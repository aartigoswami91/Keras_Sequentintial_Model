{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aartigoswami91/Keras_Sequentintial_Model/blob/main/Copy_of_Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0XmWRjzvpGKt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "#########Keras Libraries and Modules ###########\n",
        "import tensorflow as tf\n",
        "from  keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ecaBGMqRpGX3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeRMK5UstoDj",
        "outputId": "cce6659e-6425-4258-d33e-f90edb57e7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(42000, 785)\n",
            "(28000, 784)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42yZjRz2tvIW",
        "outputId": "86ae6e7a-be7f-4296-e808-76192beb6bd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label         0.026479\n",
              "pixel0        0.000000\n",
              "pixel1        0.000000\n",
              "pixel2        0.000000\n",
              "pixel3        0.000000\n",
              "               ...    \n",
              "pixel779    145.149671\n",
              "pixel780      0.000000\n",
              "pixel781      0.000000\n",
              "pixel782      0.000000\n",
              "pixel783      0.000000\n",
              "Length: 785, dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "msJFZz2nw2Z2",
        "outputId": "25de32d5-5faf-4716-cf68-b192b8369cf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d95cfdb9330>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "features = [\"pixel{}\".format(pixel_num) for pixel_num in range(0,784)]\n",
        "rows_to_examine = 150\n",
        "image_data = np.reshape(train[features][rows_to_examine:rows_to_examine+1].to_numpy(), (28,28))\n",
        "plt.imshow(image_data, cmap = 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XlzlIkrjtvKS"
      },
      "outputs": [],
      "source": [
        "X = train.iloc[:,1:]\n",
        "y = train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KMbFSvFKtvMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292c487a-d9e6-40fd-e966-7e4fa854644b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5157, 784)\n",
            "(5157,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "w1sDNR4EtvPh"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "gqRQLewEtvTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021eae40-eee9-4eec-b8c9-7e459abd1ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4125, 784) (1032, 784)\n",
            "(4125,) (1032,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "j7s7iBGPtoFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31793870-7e57-4be4-9cd1-a67b2ad836cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 20599.67, NNZs: 522, Bias: -22.000000, T: 4125, Avg. loss: 59110.557576\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 24572.71, NNZs: 531, Bias: -29.000000, T: 8250, Avg. loss: 40549.545212\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 27363.45, NNZs: 538, Bias: -39.000000, T: 12375, Avg. loss: 32586.530667\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 29737.28, NNZs: 541, Bias: -50.000000, T: 16500, Avg. loss: 31763.216970\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 33000.64, NNZs: 544, Bias: -54.000000, T: 20625, Avg. loss: 21697.116121\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 35207.98, NNZs: 551, Bias: -61.000000, T: 24750, Avg. loss: 23102.804121\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 37237.54, NNZs: 551, Bias: -68.000000, T: 28875, Avg. loss: 15496.724848\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 38968.41, NNZs: 553, Bias: -75.000000, T: 33000, Avg. loss: 15641.988364\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 40773.06, NNZs: 554, Bias: -77.000000, T: 37125, Avg. loss: 19010.872000\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 42088.24, NNZs: 554, Bias: -81.000000, T: 41250, Avg. loss: 12429.053333\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 43289.16, NNZs: 554, Bias: -82.000000, T: 45375, Avg. loss: 11818.256970\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 44828.81, NNZs: 557, Bias: -88.000000, T: 49500, Avg. loss: 8888.751273\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 45741.41, NNZs: 557, Bias: -94.000000, T: 53625, Avg. loss: 10965.424000\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 47404.08, NNZs: 557, Bias: -98.000000, T: 57750, Avg. loss: 7175.959758\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 48431.86, NNZs: 557, Bias: -101.000000, T: 61875, Avg. loss: 4782.230303\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 49769.76, NNZs: 557, Bias: -106.000000, T: 66000, Avg. loss: 9607.083152\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 50924.72, NNZs: 557, Bias: -106.000000, T: 70125, Avg. loss: 6500.621091\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 51999.52, NNZs: 557, Bias: -110.000000, T: 74250, Avg. loss: 4872.737939\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 52968.34, NNZs: 556, Bias: -114.000000, T: 78375, Avg. loss: 5929.354182\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 53566.10, NNZs: 557, Bias: -115.000000, T: 82500, Avg. loss: 4675.430545\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 54234.50, NNZs: 559, Bias: -117.000000, T: 86625, Avg. loss: 4434.066424\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 55057.63, NNZs: 558, Bias: -119.000000, T: 90750, Avg. loss: 3339.680970\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 56173.39, NNZs: 559, Bias: -122.000000, T: 94875, Avg. loss: 6938.948848\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 56397.92, NNZs: 564, Bias: -123.000000, T: 99000, Avg. loss: 2277.403152\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 56759.69, NNZs: 564, Bias: -124.000000, T: 103125, Avg. loss: 1936.091879\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 57133.07, NNZs: 564, Bias: -124.000000, T: 107250, Avg. loss: 1726.575030\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 57907.13, NNZs: 564, Bias: -127.000000, T: 111375, Avg. loss: 2926.998061\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 58545.25, NNZs: 564, Bias: -129.000000, T: 115500, Avg. loss: 4829.772121\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 58976.78, NNZs: 564, Bias: -130.000000, T: 119625, Avg. loss: 2378.574545\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 123750, Avg. loss: 1724.321697\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 127875, Avg. loss: 0.000000\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 132000, Avg. loss: 0.000000\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 136125, Avg. loss: 0.000000\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 140250, Avg. loss: 0.000000\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 144375, Avg. loss: 0.000000\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 59309.32, NNZs: 564, Bias: -131.000000, T: 148500, Avg. loss: 0.000000\n",
            "Total training time: 0.24 seconds.\n",
            "Convergence after 36 epochs took 0.24 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13301.36, NNZs: 478, Bias: -7.000000, T: 4125, Avg. loss: 33297.680000\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 16334.78, NNZs: 497, Bias: -8.000000, T: 8250, Avg. loss: 25533.969212\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17307.77, NNZs: 509, Bias: -11.000000, T: 12375, Avg. loss: 24269.691879\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 19891.63, NNZs: 516, Bias: -14.000000, T: 16500, Avg. loss: 20770.832970\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 21639.80, NNZs: 522, Bias: -15.000000, T: 20625, Avg. loss: 19228.465939\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 22793.70, NNZs: 522, Bias: -15.000000, T: 24750, Avg. loss: 17348.824000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 24967.41, NNZs: 524, Bias: -18.000000, T: 28875, Avg. loss: 16858.186909\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 26070.21, NNZs: 527, Bias: -18.000000, T: 33000, Avg. loss: 16653.661091\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 27118.51, NNZs: 528, Bias: -20.000000, T: 37125, Avg. loss: 15351.180606\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 28479.90, NNZs: 539, Bias: -21.000000, T: 41250, Avg. loss: 13817.080727\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 29805.06, NNZs: 541, Bias: -19.000000, T: 45375, Avg. loss: 11717.050909\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 31092.70, NNZs: 550, Bias: -21.000000, T: 49500, Avg. loss: 10583.619394\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 32305.59, NNZs: 550, Bias: -22.000000, T: 53625, Avg. loss: 13204.482182\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 33691.76, NNZs: 551, Bias: -22.000000, T: 57750, Avg. loss: 8625.914667\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 35316.88, NNZs: 551, Bias: -24.000000, T: 61875, Avg. loss: 9161.343030\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 35828.08, NNZs: 551, Bias: -24.000000, T: 66000, Avg. loss: 10491.501333\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 36716.28, NNZs: 551, Bias: -24.000000, T: 70125, Avg. loss: 8400.641455\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 37838.22, NNZs: 555, Bias: -28.000000, T: 74250, Avg. loss: 7724.144727\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 38907.61, NNZs: 555, Bias: -27.000000, T: 78375, Avg. loss: 7786.312242\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 39982.91, NNZs: 555, Bias: -25.000000, T: 82500, Avg. loss: 5944.001455\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 41282.52, NNZs: 565, Bias: -29.000000, T: 86625, Avg. loss: 8955.095273\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 42358.33, NNZs: 567, Bias: -32.000000, T: 90750, Avg. loss: 7691.455758\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 43208.50, NNZs: 567, Bias: -32.000000, T: 94875, Avg. loss: 6355.964848\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 44013.24, NNZs: 567, Bias: -32.000000, T: 99000, Avg. loss: 4467.553455\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 44450.36, NNZs: 567, Bias: -33.000000, T: 103125, Avg. loss: 2658.810424\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 45087.16, NNZs: 567, Bias: -35.000000, T: 107250, Avg. loss: 3060.358303\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 45841.43, NNZs: 567, Bias: -36.000000, T: 111375, Avg. loss: 5482.542303\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 46650.95, NNZs: 567, Bias: -37.000000, T: 115500, Avg. loss: 2973.968970\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 47516.14, NNZs: 567, Bias: -37.000000, T: 119625, Avg. loss: 4018.263273\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 48327.64, NNZs: 567, Bias: -38.000000, T: 123750, Avg. loss: 5469.512970\n",
            "Total training time: 0.21 seconds.\n",
            "Convergence after 30 epochs took 0.21 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21497.54, NNZs: 558, Bias: -16.000000, T: 4125, Avg. loss: 110201.040970\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 26173.84, NNZs: 577, Bias: -31.000000, T: 8250, Avg. loss: 84204.115879\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30828.61, NNZs: 585, Bias: -41.000000, T: 12375, Avg. loss: 74081.618424\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33331.50, NNZs: 591, Bias: -51.000000, T: 16500, Avg. loss: 71741.533818\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 37161.67, NNZs: 591, Bias: -66.000000, T: 20625, Avg. loss: 69382.699152\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 39866.99, NNZs: 592, Bias: -74.000000, T: 24750, Avg. loss: 59505.645333\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 42637.27, NNZs: 593, Bias: -81.000000, T: 28875, Avg. loss: 60185.943030\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 44622.89, NNZs: 599, Bias: -90.000000, T: 33000, Avg. loss: 57503.533091\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 46956.22, NNZs: 605, Bias: -100.000000, T: 37125, Avg. loss: 50515.547879\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 49644.00, NNZs: 611, Bias: -107.000000, T: 41250, Avg. loss: 47558.665939\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 51309.26, NNZs: 610, Bias: -115.000000, T: 45375, Avg. loss: 51849.366303\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 53256.88, NNZs: 612, Bias: -119.000000, T: 49500, Avg. loss: 46616.674909\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 54948.35, NNZs: 612, Bias: -126.000000, T: 53625, Avg. loss: 53149.923152\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 56653.61, NNZs: 612, Bias: -139.000000, T: 57750, Avg. loss: 47086.757818\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 58897.87, NNZs: 612, Bias: -146.000000, T: 61875, Avg. loss: 41781.278061\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 60273.98, NNZs: 613, Bias: -154.000000, T: 66000, Avg. loss: 42312.135515\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 61747.12, NNZs: 613, Bias: -161.000000, T: 70125, Avg. loss: 44641.818182\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 63773.65, NNZs: 613, Bias: -167.000000, T: 74250, Avg. loss: 37335.099636\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 64987.05, NNZs: 613, Bias: -173.000000, T: 78375, Avg. loss: 37577.458667\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 66447.07, NNZs: 613, Bias: -178.000000, T: 82500, Avg. loss: 38803.918303\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 67725.52, NNZs: 613, Bias: -184.000000, T: 86625, Avg. loss: 41626.859636\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 68834.81, NNZs: 613, Bias: -190.000000, T: 90750, Avg. loss: 36797.691879\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 70227.58, NNZs: 613, Bias: -201.000000, T: 94875, Avg. loss: 37073.835394\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 71722.89, NNZs: 613, Bias: -209.000000, T: 99000, Avg. loss: 34727.192000\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 72606.09, NNZs: 613, Bias: -211.000000, T: 103125, Avg. loss: 32292.644121\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 73792.78, NNZs: 616, Bias: -217.000000, T: 107250, Avg. loss: 30332.979394\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 74857.41, NNZs: 616, Bias: -224.000000, T: 111375, Avg. loss: 27323.595879\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 75616.28, NNZs: 616, Bias: -230.000000, T: 115500, Avg. loss: 31810.501091\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 76872.41, NNZs: 616, Bias: -237.000000, T: 119625, Avg. loss: 26189.508364\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 78238.77, NNZs: 616, Bias: -244.000000, T: 123750, Avg. loss: 34603.650667\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 79220.89, NNZs: 616, Bias: -253.000000, T: 127875, Avg. loss: 33099.239273\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 79967.00, NNZs: 616, Bias: -257.000000, T: 132000, Avg. loss: 25940.088727\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 80961.76, NNZs: 616, Bias: -262.000000, T: 136125, Avg. loss: 22184.955394\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 81717.44, NNZs: 616, Bias: -265.000000, T: 140250, Avg. loss: 23486.579636\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 82925.87, NNZs: 616, Bias: -273.000000, T: 144375, Avg. loss: 26878.354182\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 84078.16, NNZs: 616, Bias: -278.000000, T: 148500, Avg. loss: 26821.736242\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 85031.58, NNZs: 615, Bias: -286.000000, T: 152625, Avg. loss: 23049.405091\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 86182.92, NNZs: 616, Bias: -296.000000, T: 156750, Avg. loss: 23602.956364\n",
            "Total training time: 0.27 seconds.\n",
            "Convergence after 38 epochs took 0.27 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21783.44, NNZs: 541, Bias: -24.000000, T: 4125, Avg. loss: 115324.402182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27058.54, NNZs: 557, Bias: -38.000000, T: 8250, Avg. loss: 98495.588606\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30109.83, NNZs: 561, Bias: -53.000000, T: 12375, Avg. loss: 93867.431758\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 34320.91, NNZs: 565, Bias: -69.000000, T: 16500, Avg. loss: 80794.216000\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 37296.09, NNZs: 572, Bias: -86.000000, T: 20625, Avg. loss: 81536.315636\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 40033.57, NNZs: 573, Bias: -98.000000, T: 24750, Avg. loss: 75050.924848\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 41374.97, NNZs: 573, Bias: -107.000000, T: 28875, Avg. loss: 61957.774788\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 43322.50, NNZs: 573, Bias: -120.000000, T: 33000, Avg. loss: 63459.302061\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 45883.52, NNZs: 580, Bias: -133.000000, T: 37125, Avg. loss: 65620.315394\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 47872.57, NNZs: 581, Bias: -140.000000, T: 41250, Avg. loss: 61531.729212\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 49232.85, NNZs: 581, Bias: -148.000000, T: 45375, Avg. loss: 67703.142061\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 51339.20, NNZs: 583, Bias: -157.000000, T: 49500, Avg. loss: 55891.904727\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 53023.29, NNZs: 582, Bias: -167.000000, T: 53625, Avg. loss: 53168.213333\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 55073.74, NNZs: 584, Bias: -177.000000, T: 57750, Avg. loss: 57851.565091\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 56942.78, NNZs: 584, Bias: -184.000000, T: 61875, Avg. loss: 55444.654788\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 58534.04, NNZs: 584, Bias: -197.000000, T: 66000, Avg. loss: 57957.545212\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 60052.57, NNZs: 587, Bias: -210.000000, T: 70125, Avg. loss: 56767.631758\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61973.62, NNZs: 587, Bias: -222.000000, T: 74250, Avg. loss: 55669.333818\n",
            "Total training time: 0.13 seconds.\n",
            "Convergence after 18 epochs took 0.13 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21621.12, NNZs: 559, Bias: -12.000000, T: 4125, Avg. loss: 82757.981333\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 26733.38, NNZs: 579, Bias: -21.000000, T: 8250, Avg. loss: 60874.297697\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31551.38, NNZs: 584, Bias: -30.000000, T: 12375, Avg. loss: 51281.535758\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33218.40, NNZs: 588, Bias: -33.000000, T: 16500, Avg. loss: 42261.249455\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 35960.23, NNZs: 590, Bias: -40.000000, T: 20625, Avg. loss: 40460.783273\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 37970.39, NNZs: 592, Bias: -43.000000, T: 24750, Avg. loss: 36748.999758\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 39876.87, NNZs: 593, Bias: -48.000000, T: 28875, Avg. loss: 35828.595879\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 41794.84, NNZs: 593, Bias: -54.000000, T: 33000, Avg. loss: 35871.768970\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 42933.03, NNZs: 593, Bias: -56.000000, T: 37125, Avg. loss: 31737.079030\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 44786.59, NNZs: 593, Bias: -63.000000, T: 41250, Avg. loss: 30596.052364\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 45982.28, NNZs: 593, Bias: -68.000000, T: 45375, Avg. loss: 25733.255758\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 47735.29, NNZs: 594, Bias: -73.000000, T: 49500, Avg. loss: 21684.983273\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 49541.51, NNZs: 594, Bias: -79.000000, T: 53625, Avg. loss: 27394.928727\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 51178.50, NNZs: 596, Bias: -84.000000, T: 57750, Avg. loss: 21914.235152\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 52364.96, NNZs: 596, Bias: -87.000000, T: 61875, Avg. loss: 19853.342061\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 53702.03, NNZs: 596, Bias: -94.000000, T: 66000, Avg. loss: 22978.030061\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 55179.26, NNZs: 596, Bias: -97.000000, T: 70125, Avg. loss: 19726.460606\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 56857.30, NNZs: 596, Bias: -99.000000, T: 74250, Avg. loss: 22953.439758\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 57962.08, NNZs: 596, Bias: -103.000000, T: 78375, Avg. loss: 13728.220848\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 59362.32, NNZs: 597, Bias: -107.000000, T: 82500, Avg. loss: 18379.313939\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 60854.39, NNZs: 596, Bias: -112.000000, T: 86625, Avg. loss: 17682.887515\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 62723.66, NNZs: 597, Bias: -117.000000, T: 90750, Avg. loss: 22213.789818\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 63821.55, NNZs: 597, Bias: -123.000000, T: 94875, Avg. loss: 15171.701333\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 64874.50, NNZs: 597, Bias: -125.000000, T: 99000, Avg. loss: 16728.600485\n",
            "Total training time: 0.17 seconds.\n",
            "Convergence after 24 epochs took 0.17 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22727.15, NNZs: 546, Bias: -5.000000, T: 4125, Avg. loss: 150026.146182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 30024.43, NNZs: 562, Bias: 2.000000, T: 8250, Avg. loss: 121772.945212\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 33773.31, NNZs: 566, Bias: 3.000000, T: 12375, Avg. loss: 106283.353697\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 36896.51, NNZs: 567, Bias: 9.000000, T: 16500, Avg. loss: 96798.459636\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 40337.38, NNZs: 569, Bias: 9.000000, T: 20625, Avg. loss: 89942.872485\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 42780.28, NNZs: 570, Bias: 14.000000, T: 24750, Avg. loss: 87451.255030\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 45584.19, NNZs: 570, Bias: 18.000000, T: 28875, Avg. loss: 89286.214303\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 48580.20, NNZs: 571, Bias: 22.000000, T: 33000, Avg. loss: 85236.039273\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 50091.17, NNZs: 571, Bias: 24.000000, T: 37125, Avg. loss: 88306.111273\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 52127.18, NNZs: 574, Bias: 27.000000, T: 41250, Avg. loss: 85084.172364\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 54012.08, NNZs: 574, Bias: 26.000000, T: 45375, Avg. loss: 80576.290667\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 56647.84, NNZs: 575, Bias: 29.000000, T: 49500, Avg. loss: 72134.901091\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 58544.86, NNZs: 576, Bias: 28.000000, T: 53625, Avg. loss: 78265.653818\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60534.08, NNZs: 576, Bias: 32.000000, T: 57750, Avg. loss: 62359.424242\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 62341.85, NNZs: 576, Bias: 33.000000, T: 61875, Avg. loss: 74666.671273\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 63752.33, NNZs: 576, Bias: 38.000000, T: 66000, Avg. loss: 67943.445333\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 65115.61, NNZs: 576, Bias: 38.000000, T: 70125, Avg. loss: 65657.801455\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 66551.40, NNZs: 576, Bias: 42.000000, T: 74250, Avg. loss: 59559.679515\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 67953.32, NNZs: 576, Bias: 43.000000, T: 78375, Avg. loss: 58477.743030\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 69519.52, NNZs: 576, Bias: 43.000000, T: 82500, Avg. loss: 65545.079515\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 71502.67, NNZs: 576, Bias: 51.000000, T: 86625, Avg. loss: 62010.869818\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 72694.96, NNZs: 576, Bias: 53.000000, T: 90750, Avg. loss: 54843.287758\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 74254.53, NNZs: 576, Bias: 51.000000, T: 94875, Avg. loss: 57084.345455\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 76080.66, NNZs: 576, Bias: 51.000000, T: 99000, Avg. loss: 60111.033697\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 78036.80, NNZs: 576, Bias: 52.000000, T: 103125, Avg. loss: 67251.602667\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 79695.89, NNZs: 576, Bias: 50.000000, T: 107250, Avg. loss: 55341.068121\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 81552.61, NNZs: 578, Bias: 49.000000, T: 111375, Avg. loss: 63325.744000\n",
            "Total training time: 0.19 seconds.\n",
            "Convergence after 27 epochs took 0.19 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20405.32, NNZs: 534, Bias: -14.000000, T: 4125, Avg. loss: 73277.632727\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 25891.31, NNZs: 546, Bias: -30.000000, T: 8250, Avg. loss: 55755.440970\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 27225.74, NNZs: 547, Bias: -42.000000, T: 12375, Avg. loss: 52040.601212\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 29504.79, NNZs: 552, Bias: -51.000000, T: 16500, Avg. loss: 46567.711273\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 31653.98, NNZs: 555, Bias: -60.000000, T: 20625, Avg. loss: 44163.171394\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 33337.49, NNZs: 556, Bias: -70.000000, T: 24750, Avg. loss: 40744.413818\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 35358.07, NNZs: 558, Bias: -80.000000, T: 28875, Avg. loss: 37419.762182\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 37803.16, NNZs: 558, Bias: -90.000000, T: 33000, Avg. loss: 37563.165818\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 39922.84, NNZs: 558, Bias: -102.000000, T: 37125, Avg. loss: 36707.703030\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 41869.15, NNZs: 559, Bias: -112.000000, T: 41250, Avg. loss: 34269.343758\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 43138.65, NNZs: 561, Bias: -119.000000, T: 45375, Avg. loss: 30231.692848\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 44683.93, NNZs: 560, Bias: -128.000000, T: 49500, Avg. loss: 31306.791030\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 45683.06, NNZs: 562, Bias: -135.000000, T: 53625, Avg. loss: 31065.494788\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 47168.18, NNZs: 565, Bias: -141.000000, T: 57750, Avg. loss: 24835.492121\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 48673.90, NNZs: 565, Bias: -150.000000, T: 61875, Avg. loss: 26676.490182\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 50394.28, NNZs: 566, Bias: -155.000000, T: 66000, Avg. loss: 24983.070545\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 51301.14, NNZs: 566, Bias: -162.000000, T: 70125, Avg. loss: 20951.112242\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 52628.95, NNZs: 566, Bias: -171.000000, T: 74250, Avg. loss: 29521.119515\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 53755.34, NNZs: 565, Bias: -179.000000, T: 78375, Avg. loss: 22733.542788\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 55216.44, NNZs: 566, Bias: -183.000000, T: 82500, Avg. loss: 21416.604606\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 56480.93, NNZs: 564, Bias: -190.000000, T: 86625, Avg. loss: 19345.971636\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 56929.78, NNZs: 566, Bias: -193.000000, T: 90750, Avg. loss: 16645.768727\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 58240.32, NNZs: 566, Bias: -202.000000, T: 94875, Avg. loss: 21605.561212\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 59173.43, NNZs: 565, Bias: -211.000000, T: 99000, Avg. loss: 20643.469576\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 60677.75, NNZs: 566, Bias: -216.000000, T: 103125, Avg. loss: 16358.341091\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 62062.72, NNZs: 566, Bias: -221.000000, T: 107250, Avg. loss: 22195.374061\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 63195.55, NNZs: 566, Bias: -229.000000, T: 111375, Avg. loss: 20936.148606\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 63935.92, NNZs: 566, Bias: -234.000000, T: 115500, Avg. loss: 17054.225939\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 65157.49, NNZs: 566, Bias: -241.000000, T: 119625, Avg. loss: 16324.486061\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 66360.08, NNZs: 566, Bias: -250.000000, T: 123750, Avg. loss: 19387.034182\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 67289.91, NNZs: 566, Bias: -255.000000, T: 127875, Avg. loss: 16524.502788\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 68773.54, NNZs: 566, Bias: -261.000000, T: 132000, Avg. loss: 16768.726303\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 69460.59, NNZs: 566, Bias: -266.000000, T: 136125, Avg. loss: 17768.200000\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 70230.60, NNZs: 566, Bias: -273.000000, T: 140250, Avg. loss: 16389.828364\n",
            "Total training time: 0.23 seconds.\n",
            "Convergence after 34 epochs took 0.23 seconds\n",
            "-- Epoch 1\n",
            "Norm: 18358.98, NNZs: 526, Bias: -2.000000, T: 4125, Avg. loss: 66586.151030\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 21724.11, NNZs: 547, Bias: -6.000000, T: 8250, Avg. loss: 56239.611152\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 26319.37, NNZs: 548, Bias: -8.000000, T: 12375, Avg. loss: 50270.129212\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 29665.72, NNZs: 551, Bias: -10.000000, T: 16500, Avg. loss: 42426.997091\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 32504.39, NNZs: 551, Bias: -13.000000, T: 20625, Avg. loss: 36256.811152\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 34825.11, NNZs: 551, Bias: -15.000000, T: 24750, Avg. loss: 31898.589576\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 36657.03, NNZs: 551, Bias: -16.000000, T: 28875, Avg. loss: 28650.972606\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 39407.49, NNZs: 552, Bias: -23.000000, T: 33000, Avg. loss: 28494.839273\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 41293.53, NNZs: 552, Bias: -23.000000, T: 37125, Avg. loss: 28093.932848\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 42669.43, NNZs: 552, Bias: -25.000000, T: 41250, Avg. loss: 20789.920970\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 44126.90, NNZs: 552, Bias: -28.000000, T: 45375, Avg. loss: 28118.461333\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 45966.13, NNZs: 552, Bias: -33.000000, T: 49500, Avg. loss: 22907.021576\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 47392.65, NNZs: 552, Bias: -33.000000, T: 53625, Avg. loss: 19854.288485\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 48770.46, NNZs: 552, Bias: -36.000000, T: 57750, Avg. loss: 20443.785939\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 50172.98, NNZs: 552, Bias: -36.000000, T: 61875, Avg. loss: 20706.336485\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 51133.73, NNZs: 552, Bias: -39.000000, T: 66000, Avg. loss: 16216.053333\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 52417.74, NNZs: 551, Bias: -41.000000, T: 70125, Avg. loss: 16513.424727\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 53768.71, NNZs: 551, Bias: -46.000000, T: 74250, Avg. loss: 17993.475879\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 55401.90, NNZs: 552, Bias: -45.000000, T: 78375, Avg. loss: 15221.765576\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 56525.68, NNZs: 556, Bias: -44.000000, T: 82500, Avg. loss: 13137.232970\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 57668.88, NNZs: 563, Bias: -45.000000, T: 86625, Avg. loss: 13363.320970\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 58892.60, NNZs: 563, Bias: -44.000000, T: 90750, Avg. loss: 16338.177939\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 60150.36, NNZs: 565, Bias: -49.000000, T: 94875, Avg. loss: 11497.422788\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 61042.51, NNZs: 565, Bias: -52.000000, T: 99000, Avg. loss: 8882.338667\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 62007.27, NNZs: 565, Bias: -53.000000, T: 103125, Avg. loss: 11509.630303\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 63321.25, NNZs: 567, Bias: -54.000000, T: 107250, Avg. loss: 11555.296727\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 64042.48, NNZs: 569, Bias: -53.000000, T: 111375, Avg. loss: 9106.982545\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 64781.96, NNZs: 569, Bias: -51.000000, T: 115500, Avg. loss: 9964.755879\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 65717.87, NNZs: 569, Bias: -54.000000, T: 119625, Avg. loss: 12262.648970\n",
            "Total training time: 0.19 seconds.\n",
            "Convergence after 29 epochs took 0.19 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22954.85, NNZs: 542, Bias: -59.000000, T: 4125, Avg. loss: 222548.917818\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27542.74, NNZs: 559, Bias: -107.000000, T: 8250, Avg. loss: 187804.075152\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32843.97, NNZs: 567, Bias: -150.000000, T: 12375, Avg. loss: 178245.242182\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 35632.85, NNZs: 576, Bias: -189.000000, T: 16500, Avg. loss: 195828.241455\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 40666.10, NNZs: 580, Bias: -233.000000, T: 20625, Avg. loss: 181422.488000\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 42878.34, NNZs: 581, Bias: -270.000000, T: 24750, Avg. loss: 173442.358788\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 45722.14, NNZs: 581, Bias: -314.000000, T: 28875, Avg. loss: 173145.660121\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 47943.41, NNZs: 588, Bias: -351.000000, T: 33000, Avg. loss: 165122.590303\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 50530.17, NNZs: 588, Bias: -390.000000, T: 37125, Avg. loss: 166297.972121\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 52662.12, NNZs: 588, Bias: -424.000000, T: 41250, Avg. loss: 164871.569212\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 55078.93, NNZs: 588, Bias: -464.000000, T: 45375, Avg. loss: 158523.388848\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 56967.48, NNZs: 591, Bias: -501.000000, T: 49500, Avg. loss: 153027.319030\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 58954.17, NNZs: 591, Bias: -539.000000, T: 53625, Avg. loss: 148663.680000\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 60573.68, NNZs: 591, Bias: -574.000000, T: 57750, Avg. loss: 151672.216485\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 63917.63, NNZs: 592, Bias: -615.000000, T: 61875, Avg. loss: 152969.970909\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 65294.22, NNZs: 592, Bias: -651.000000, T: 66000, Avg. loss: 148489.387636\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 67826.83, NNZs: 591, Bias: -686.000000, T: 70125, Avg. loss: 152174.289939\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 69775.80, NNZs: 594, Bias: -726.000000, T: 74250, Avg. loss: 153085.269333\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 71824.49, NNZs: 595, Bias: -761.000000, T: 78375, Avg. loss: 145153.457212\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 73508.15, NNZs: 595, Bias: -801.000000, T: 82500, Avg. loss: 141632.351515\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 75099.27, NNZs: 595, Bias: -838.000000, T: 86625, Avg. loss: 142303.428606\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 77202.65, NNZs: 595, Bias: -875.000000, T: 90750, Avg. loss: 142247.488242\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 78447.50, NNZs: 595, Bias: -908.000000, T: 94875, Avg. loss: 152957.278545\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 80284.97, NNZs: 600, Bias: -942.000000, T: 99000, Avg. loss: 134784.453091\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 81961.52, NNZs: 600, Bias: -984.000000, T: 103125, Avg. loss: 145697.743030\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 83664.96, NNZs: 600, Bias: -1019.000000, T: 107250, Avg. loss: 149556.263030\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 85116.01, NNZs: 600, Bias: -1052.000000, T: 111375, Avg. loss: 144674.744000\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 86674.51, NNZs: 600, Bias: -1087.000000, T: 115500, Avg. loss: 137886.483394\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 88114.97, NNZs: 600, Bias: -1118.000000, T: 119625, Avg. loss: 137215.845091\n",
            "Total training time: 0.21 seconds.\n",
            "Convergence after 29 epochs took 0.21 seconds\n",
            "-- Epoch 1\n",
            "Norm: 20984.64, NNZs: 555, Bias: -21.000000, T: 4125, Avg. loss: 166294.187152\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 25924.88, NNZs: 581, Bias: -42.000000, T: 8250, Avg. loss: 149993.038061\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31138.73, NNZs: 590, Bias: -68.000000, T: 12375, Avg. loss: 146027.082667\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 34487.49, NNZs: 601, Bias: -88.000000, T: 16500, Avg. loss: 139262.534303\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 37018.71, NNZs: 603, Bias: -96.000000, T: 20625, Avg. loss: 118101.443394\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 40035.19, NNZs: 603, Bias: -113.000000, T: 24750, Avg. loss: 113153.779636\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 42070.21, NNZs: 603, Bias: -128.000000, T: 28875, Avg. loss: 119127.998788\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 44396.22, NNZs: 603, Bias: -137.000000, T: 33000, Avg. loss: 110978.423515\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 46694.37, NNZs: 603, Bias: -151.000000, T: 37125, Avg. loss: 120116.735758\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 49073.66, NNZs: 603, Bias: -165.000000, T: 41250, Avg. loss: 107856.432485\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 51565.82, NNZs: 606, Bias: -181.000000, T: 45375, Avg. loss: 122505.104485\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 53531.73, NNZs: 606, Bias: -192.000000, T: 49500, Avg. loss: 111425.699394\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 55325.23, NNZs: 606, Bias: -207.000000, T: 53625, Avg. loss: 105057.401697\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 57253.97, NNZs: 606, Bias: -221.000000, T: 57750, Avg. loss: 107335.474424\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 59531.49, NNZs: 606, Bias: -234.000000, T: 61875, Avg. loss: 101056.903515\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 61369.79, NNZs: 607, Bias: -249.000000, T: 66000, Avg. loss: 105396.498424\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 63221.87, NNZs: 607, Bias: -257.000000, T: 70125, Avg. loss: 114214.482667\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 65155.22, NNZs: 607, Bias: -271.000000, T: 74250, Avg. loss: 99464.381576\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 67167.80, NNZs: 607, Bias: -289.000000, T: 78375, Avg. loss: 100722.132606\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 68823.95, NNZs: 607, Bias: -304.000000, T: 82500, Avg. loss: 103480.329212\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 70794.70, NNZs: 607, Bias: -307.000000, T: 86625, Avg. loss: 92983.797091\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 72785.73, NNZs: 607, Bias: -320.000000, T: 90750, Avg. loss: 91283.854303\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 74710.55, NNZs: 607, Bias: -330.000000, T: 94875, Avg. loss: 88510.752000\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 76094.78, NNZs: 607, Bias: -343.000000, T: 99000, Avg. loss: 86848.970667\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 77359.95, NNZs: 607, Bias: -350.000000, T: 103125, Avg. loss: 95093.556121\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 78584.54, NNZs: 607, Bias: -364.000000, T: 107250, Avg. loss: 95840.140848\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 80379.73, NNZs: 607, Bias: -376.000000, T: 111375, Avg. loss: 92060.052606\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 81557.96, NNZs: 607, Bias: -385.000000, T: 115500, Avg. loss: 94560.140121\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 82840.76, NNZs: 607, Bias: -396.000000, T: 119625, Avg. loss: 91209.857455\n",
            "Total training time: 0.20 seconds.\n",
            "Convergence after 29 epochs took 0.20 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2d8a19abdcdc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mper_preds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mper_preds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPerceptron does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "per = Perceptron(verbose=2)\n",
        "per.fit(X_train, y_train)\n",
        "per_preds_train = per.predict(X_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "L5VMOhLetoIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "423b174a-3854-4e1b-ab1f-d8595e028546"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-01908b3925de>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mper_preds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \"\"\"\n\u001b[1;32m    418\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPerceptron does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZF01muoWtoM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8311a35-cbbc-440a-87a2-3b27a3010cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n"
          ]
        }
      ],
      "source": [
        "print(per.n_iter_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FT40PKqhpCx",
        "outputId": "16f4bdf5-534b-4208-fba7-1a96954dc127"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per.loss_function_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4re_XmUhpF9",
        "outputId": "ee634a6c-dc67-4953-e23c-f1d4489e57de"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.linear_model._sgd_fast.Hinge at 0x7d95ce237770>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(actual,predicted):\n",
        "  conf_matrix = confusion_matrix(actual,predicted)\n",
        "  class_rep = classification_report(actual,predicted)\n",
        "  acc_score = accuracy_score(actual,predicted)\n",
        "  print(conf_matrix)\n",
        "  print(acc_score)\n",
        "  print(class_rep)"
      ],
      "metadata": {
        "id": "00x8i9DFioCo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_train,per_preds_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKV1ypdeioFt",
        "outputId": "3a38f1c0-ed47-40b8-ea4e-40889c8d8cbd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[398   0   1   0   0   0   0   0   0   2]\n",
            " [  0 468   0   0   0   0   0   0   0   1]\n",
            " [  0   4 428   0   3   1   0   0   1   1]\n",
            " [  0   5   7 346   0  21   0   0   4  14]\n",
            " [  0   0   1   0 379   0   0   0   0   7]\n",
            " [  0   2   4   1   6 358   0   0   3   7]\n",
            " [  3   2   3   0   9   0 412   0   2   5]\n",
            " [  0   0   2   0   8   0   0 336   1  82]\n",
            " [  4  19   6   6   8  26   0   0 291  29]\n",
            " [  1   1   0   0   7   1   0   1   0 387]]\n",
            "0.921939393939394\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       401\n",
            "           1       0.93      1.00      0.96       469\n",
            "           2       0.95      0.98      0.96       438\n",
            "           3       0.98      0.87      0.92       397\n",
            "           4       0.90      0.98      0.94       387\n",
            "           5       0.88      0.94      0.91       381\n",
            "           6       1.00      0.94      0.97       436\n",
            "           7       1.00      0.78      0.88       429\n",
            "           8       0.96      0.75      0.84       389\n",
            "           9       0.72      0.97      0.83       398\n",
            "\n",
            "    accuracy                           0.92      4125\n",
            "   macro avg       0.93      0.92      0.92      4125\n",
            "weighted avg       0.93      0.92      0.92      4125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pg4C6LCWioyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALbY640hio0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MKHaIy9io4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mn9RaoSVhpLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}